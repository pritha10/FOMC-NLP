---
title: "Topic Modelling FOMC Minutes 1993-2019"
author: "Pritha Chaudhuri"
date: "5/15/2020"
output:
  html_document:
    code_folding: hide
---

<style> p.caption {font-weight: bold;} </style>

<div style="margin-bottom:100px;">

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE)
```

```{r}
library(pacman) # package to install and load packages with one command
p_load(tidyverse,lubridate,xml2,rvest,tidytext,topicmodels,ggplot2,
       tm,SnowballC,wordcloud,doParallel,stargazer,fredr,Quandl,vars,forecast)
set.seed(123)
select <- dplyr::select
```

### FOMC minutes corpus
```{r, eval=FALSE}
## Extract FOMC statements from web 
# Get dates and minutes URL from csv file
dates <- read.csv("data/minurl.csv") %>%
  mutate(mdate=ymd(mdate),url=as.character(url))

# Minutes url changes over time
# First part of minutes include names of FOMC members and those attending meeting
# Using selector gadget find nodes to select part of page needed
dates <- dates%>%
  mutate(year = as.numeric(year(dates$mdate)),
         node1 = "blockquote , table~ p",
         node2 = "p",
         node3 = "blockquote~ p , blockquote p , p:nth-child(31)")

# Function extracts text from URl and nodes
# Returns error if above nodes don't work
getFomc <- function(mdate,url,node1,node2){
  x <- html_nodes(read_html(url),node1)
  if (length(x)>=1){#good
  }else{
    x <- html_nodes(read_html(url),node2)
    if (length(x)>=1) {
      #good
    }else{cat('Check node for url:',url)}
  }
  x <- html_text(x)
  return(tibble(date=mdate,text=x))
}

# Creating corpus  
fomc_corp <- tibble()
for(i in 1:nrow(dates)){
  cat(i,'/',nrow(dates),'\n')
  fomc_corp <- bind_rows(fomc_corp,getFomc(dates$mdate[i],dates$url[i],dates$node1[i],dates$node2[i]))
}

data.table::fwrite(fomc_corp, file = "fomc1993_2019.csv")

head(fomc_corp)
```

#### Data Preprocessing
The FOMC corpus is tokenized at the word level (as unigrams). Pre-processing the corpus includes removing stopwords, such as "a", "an", "the" or other commonly used words that appear regularly in a corpus. The list is the 'stop_words' data frame included in the tidytext package. To this list I add my own list of non-useful words, that appear often in a FOMC statement but are not meaningful to build topics. Some of these words include "month", "quarter", "chairman", "system", etc. Numbers and blank spaces are also removed. Words are then stemmed using the Porter algorithm, reducing words to their common root (for example, "accommodation" and "accommodate" both become "accommod"). Pre-processing the corpus is done to obtain more meaningful topics. 

```{r}
# If corpus has been created from before, read from csv file
fomc_corp <- read.csv("fomc1993_2019.csv")

# tokenize corpus at word level (unigram)
fomc_word <- fomc_corp %>% 
  # mutate(paraid = 1:n()) %>% 
  unnest_tokens(output = word,
                input = text,
                token = 'words')

# Function to pre-process corpus to get more meaningful topics
clean_text <- function(fomc_word){
  # Remove stop words, list comes from tidytext data frame
  # Some other non-useful words added to list of stopwords
  new_stop_words <- stop_words
  nonuseful_words <- c("month", "year", "quarter", "period", "january", "february", "march",
                       "april", "may", "june", "july", "august", "september", "october", 
                       "november", "december", "system", "reserve", "participants", "open",
                       "chairman", "federal", "also", "meeting", "FOMC", "-", "\r", "\n", 
                       ".", "york", "a.m", "i.e.", "i.", "ii.", "ii", "greenspan",
                       "iv")
  nonuseful_words <- tibble(word = nonuseful_words)
  new_stop_words <- bind_rows(new_stop_words, nonuseful_words)
  
  fomc_word <- anti_join(fomc_word, new_stop_words, by = 'word')
  
  # remove numbers
  fomc_word <- fomc_word %>% 
    mutate(word = gsub("[[:digit:]]+","",word)) %>% 
    filter(word != "")
  
  # stem words using Porter algorithm, SnowballC package
  fomc_word_stem <- wordStem(fomc_word$word, language = "porter")
  fomc_word_stem <- tibble(date = fomc_word$date,
                           word = fomc_word_stem)
  # fomc_word_stem <- tibble(date = fomc_word$date,
  #                          paragraph = fomc_word$paragraph,
  #                          para_id = fomc_word$para_id,
  #                          word = fomc_word_stem)
  
  # remove stop words for a second time, applied to some words introduced after stemming
  # fomc_word_stem <- anti_join(fomc_word_stem, new_stop_words, by = "word")
  
  fomc_word_final <- anti_join(fomc_word_stem, new_stop_words, by = "word") %>% filter(word != "")
  
  return(fomc_word_final)
}

fomc_word_final <- clean_text(fomc_word) %>% 
  group_by(date, word) %>% 
  summarise(n = n()) %>% 
  ungroup()
```

The final corpus is at the document-word level including count of each word in a document 
```{r}
head(fomc_word_final)
```


**Top 10 words in FOMC minutes** 

```{r}
fomc_top10 <- mutate(fomc_word_final,year=year(date))%>%
  group_by(year)%>%
  top_n(10,n)

fomc_top10%>%
  ggplot(aes(word, n, label=word)) +
  geom_col(show.legend = F) +
  facet_wrap(~year, scales = 'free') +
  coord_flip() + 
  scale_x_reordered() +
  theme_minimal()
```

### Topic Modelling {.tabset}
To extract topics from FOMC minutes I use LDA algorithm. The corpus needs to be cast into a document-term-matrix format. The LDA algorithm provides 2 measures: $\beta$ measuring frequency of each word in a topic and $\gamma$ the frequency of each topic in a document. I arbitrarily select the number of topics to estimate as 10 for now. In the future, I will use cross-validation to pick the optimum number of topics. 

```{r}
# Create Document-term matrix for LDA
fomc_dtm <- cast_dtm(fomc_word_final, document = date, term = word, value = n)

# Select arbitrary number of topics for now
# Later will use cross-validation to obtain the optimum number of topics 
k <- 10

# Run LDA 
# Get word frequency by topic (beta) and topic frequency by document (gamma) in tidy format
fomc_lda <- LDA(fomc_dtm, k, method = "Gibbs")
topics_beta <- tidy(fomc_lda, matrix = "beta")
topics_gamma <- tidy(fomc_lda, matrix = "gamma")
```

#### Top 20 words in each topic

```{r}
top_terms <- topics_beta %>% 
  group_by(topic) %>% 
  top_n(20, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)
# visualize
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) + 
  geom_col(show.legend = F) + 
  facet_wrap(~topic, scales = 'free') + 
  coord_flip() + 
  scale_x_reordered()
```


#### Topic proportions over time

```{r}
topics_gamma %>% 
  mutate(year = year(document)) %>% 
  group_by(year, topic) %>% 
  summarise(gamma = mean(gamma)) %>% 
  ggplot(aes(year, gamma, fill = factor(topic))) + 
  geom_area(position = 'stack') +
  geom_vline(xintercept = 2009)+
  facet_wrap(~topic) +
  theme_minimal() +
  xlab("Meetings") + 
  ylab("Proportion of Document") + 
  theme(legend.title = element_blank())

```

Taking a closer look at some of the topics that spiked around the different QE programs

#### Topic 7

```{r}
topics_gamma %>% 
  filter(topic %in% c(7)) %>% 
  mutate(doc = as.Date.character(document, tryFormats = "%Y-%m-%d")) %>% 
  ggplot(aes(doc, gamma, group = 1)) + 
  geom_line(linetype = "solid", color = "blue", size = 1, show.legend = F) +
  # geom_vline(xintercept = 2009, size = 0.25) +
  theme_minimal() +
  xlab("Meetings") +
  ylab("Proportion of Document") +
  scale_x_date(date_breaks = "2 years", date_labels = "%y") +
  theme(legend.title = element_blank(),
        text = element_text(size = 20),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"),
        axis.title.x = element_blank())
```

<!-- **Topic 8** -->

<!-- ```{r} -->
<!-- topics_gamma %>%  -->
<!--   filter(topic %in% c(8)) %>%  -->
<!--   mutate(doc = as.Date.character(document, tryFormats = "%Y-%m-%d")) %>%  -->
<!--   ggplot(aes(doc, gamma, group = 1)) +  -->
<!--   geom_line(linetype = "solid", color = "red", size = 1, show.legend = F) + -->
<!--   # geom_vline(xintercept = 2009, size = 0.25) + -->
<!--   theme_minimal() + -->
<!--   xlab("Meetings") + -->
<!--   ylab("Proportion of Document") + -->
<!--   scale_x_date(date_breaks = "2 years", date_labels = "%y") + -->
<!--   theme(legend.title = element_blank(), -->
<!--         text = element_text(size = 20), -->
<!--         panel.grid.major = element_blank(), -->
<!--         panel.grid.minor = element_blank(), -->
<!--         axis.line = element_line(colour = "black"), -->
<!--         axis.title.x = element_blank()) -->
<!-- ``` -->

#### Topic 10

```{r}
topics_gamma %>% 
  filter(topic %in% c(10)) %>% 
  mutate(doc = as.Date.character(document, tryFormats = "%Y-%m-%d")) %>% 
  ggplot(aes(doc, gamma, group = 1)) + 
  geom_line(linetype = "solid", color = "red", size = 1, show.legend = F) +
  # geom_vline(xintercept = 2009, size = 0.25) +
  theme_minimal() +
  xlab("Meetings") +
  ylab("Proportion of Document") +
  scale_x_date(date_breaks = "2 years", date_labels = "%y") +
  theme(legend.title = element_blank(),
        text = element_text(size = 20),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"),
        axis.title.x = element_blank())
```

### Index for LSAP {.tabset}
The index for Fed's large-scale asset purchase program measures how much of each FOMC meeting/statement was dedicated to discussing the program. Using the $\beta$ and $\gamma$ measures from LDA I can calculate the fraction of each document dedicated to LSAP. 

To do this first I make a list of 12 most common words that are associated with this program. Then using the $\beta$ matrix I calculate the proportion of each topic that is explained by the above list of words $\theta^i$ as follows 

$$ \theta^i = \beta^i_1 + \beta^i_2 + \cdots + \beta^i_{12} $$

where $\beta^i_1$ is the proportion of word 1 in topic $i$ and so on. 

#### LSAP word prop by topic
```{r}
# List of words often used for unconventional policy such as LSAP
index_words <- c("treasury", "agency", "mortgage", "maturity", "financial","accommodation",
                 "credit", "asset", "purchase", "balance", "sheet", "housing")
# Stem words using Porter algorithm
index_words_stem <- wordStem(index_words, language = "porter")
index_words_stem <- tibble(word = index_words_stem)

# Prop of each topic that is explained by above list of words
index_beta <- inner_join(topics_beta, index_words_stem, by = c("term" = "word")) %>% 
  group_by(topic) %>% 
  summarise(prop = 100*sum(beta))

index_beta
```

Above results show words make up 10.99% of topic 7 and 10.31% of topic 10. Let's take a look at the top 20 words in each of these topics. 

#### Top 20 words
```{r}
top_terms %>% 
  filter(topic %in% c(7,10)) %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) + 
  geom_col(show.legend = F) + 
  scale_fill_manual(values = c("blue", "red")) +
  facet_wrap(~topic, scales = 'free') + 
  coord_flip() + 
  scale_x_reordered() +
  theme_minimal() + 
  labs(x = NULL, y = "Word proportions") + 
  theme(text = element_text(size = 15),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"))
```

Looking at the proportion of these topics and the top 20 words in these topics, I conclude that topics 7 and 10 correspond to Fed's LSAP program. I will use topic 7 and 10 proportions for each document ($\gamma$s) and the word proportions created above ($\theta$s) to create the share of each document (or meeting) 
 dedicated to LSAP as
 
$$ \text{Index}_d = \gamma^3_d\theta^3 + \gamma^8_d\theta^8 + \gamma^{10}_d\theta^{10} $$

#### LSAP Index
```{r}
index_lsap <- topics_gamma %>% 
  arrange(document) %>%
  left_join(index_beta, by = "topic") %>% 
  filter(topic %in% c(7,10)) %>% 
  group_by(document) %>% 
  summarise(indexlsap = sum(gamma*prop)) %>%
  mutate(doc = as.Date.character(document, tryFormats = "%Y-%m-%d"),
         year = year(doc), 
         month = month(doc))
  

index_lsap %>% 
  ggplot(aes(doc, indexlsap, group = 1)) +
  geom_line(linetype = "solid", color = "black", size = 1, show.legend = F) +
  theme_minimal() +
  xlab("Meetings") +
  ylab("Index") +
  scale_x_date(date_breaks = "4 years", date_labels = "%b-%y") +
  theme(legend.title = element_blank(),
        text = element_text(size = 20),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"),
        axis.title.x = element_blank())
```

### Time Series Regression
To test that the index is a measure of LSAP I can study its impact on long-term rates, fr example the 10-year Treasury rate. This can be done through simple OLS 

$$ i^{10}_t = \alpha*\text{Index}_t + X_t\delta + \epsilon_t $$

where $i^{10}_t$ is the 10-year Treasury rate. $\alpha$ is the coefficient of interest as it measures the effect of the LSAP index on the 10-year Treasury. $X_t$ is a list of controls that include industrial production, 12-month change in core CPI for an inflation measure and unemployment rate. FOr the short-term interest rate measure I use three different rates: the Federal Funds Rate, the 3-month Treasury rate and the 1-year Treasury rate. 

```{r}
get_data <- function(tsregdata, start, end, index_lsap){
  tsmacro <- list()
  for (i in tsregdata){
    tsmacro[[i]] <- read.csv(paste0("macro_variables/",i,".csv"), header = T, stringsAsFactors = F) %>% 
      mutate(Date = as.Date.character(DATE, tryFormats = "%Y-%m-%d"),
             year = year(Date), 
             month = month(Date)) %>% 
      filter(year %in% c(start:end)) %>% 
      select(year, month, if_else(i=="CPIAUCSL", paste0(i,"_PC1"), i))
  }
  
  regdata <- reduce(tsmacro,left_join,by=c("year","month")) %>% 
    left_join(index_lsap, by = c("year", "month")) %>% 
    mutate(lag = na.locf(indexlsap, na.rm = F),
           index_lsap = if_else(is.na(indexlsap), round(lag, 4), round(indexlsap, 4)),
           index_lsap = if_else(is.na(index_lsap), 0, index_lsap)) %>% 
    select("year", "month", "INDPRO", "UNRATE", "CPIAUCSL_PC1", "GS10", "GS1", "GS3M", "FEDFUNDS", "index_lsap", "GS20", "GS30")
  
  # regdata <- tsmacro[[1]]
  # for (i in 2:length(tsmacro)){
  #   regdata <- inner_join(regdata,tsmacro[[i]],by=c("year","month"))
  # }
  
  # regdata <- bind_cols(tsmacro) %>% 
  #   select("year", "month", "INDPRO", "UNRATE", "CPIAUCSL_PC1", "GS10", "GS1", "GS3M", "FEDFUNDS", "GS20", "GS30") %>% 
  #   left_join(index_lsap, by = c("year", "month")) %>% 
  #   mutate(lag = na.locf(indexlsap, na.rm = F),
  #          index_lsap = if_else(is.na(indexlsap), round(lag, 4), round(indexlsap, 4)),
  #          index_lsap = if_else(is.na(index_lsap), 0, index_lsap)) %>% 
  #   select("year", "month", "INDPRO", "UNRATE", "CPIAUCSL_PC1", "GS10", "GS1", "GS3M", "FEDFUNDS", "index_lsap", "GS20", "GS30")
    
  return(regdata)
  
}

run_reg <- function(y,x,data){
  form <- as.formula(paste0(y,"~",paste(x, collapse = "+")))
  lm <- lm(form, data)
}

tsregdata <- c("INDPRO", "UNRATE", "CPIAUCSL", "GS10", "GS1", "GS3M", "FEDFUNDS", "GS20", "GS30")
regdata <- get_data(tsregdata, 1993, 2019, index_lsap)
lm.results <- list()
var_reg <- c("INDPRO", "CPIAUCSL_PC1", "UNRATE", "index_lsap")
lm.results <- lapply(c("FEDFUNDS", "GS1", "GS3M"), function(x) run_reg("GS10", c(var_reg,x), regdata))
stargazer(lm.results, type = "text")

```

A one unit increase in the LSAP index reduces the 10-year Treasury rate by approximately 0.06 pp, regardless of the short-term rate used. Since an indended effect of the program was a reduction in long-term rates, results confirm this LSAP index is able to explain movement of long-term rates and is a good proxy for the Fed's LSAP program. 

```{r}
# lasp-noise
lm.lsap_orth <- run_reg("index_lsap",c("INDPRO", "CPIAUCSL_PC1", "UNRATE","FEDFUNDS"), regdata)
regdata$lsap_hat <- predict(lm.lsap_orth,regdata)


lm.lsap <- run_reg("GS10","index_lsap", regdata)
lm.lsap2 <- run_reg("GS10","lsap_hat", regdata)

regdata$y_hat <- predict(lm.lsap,regdata)
regdata$y_hat2 <- predict(lm.lsap2,regdata)

stargazer(lm.lsap,lm.lsap2, type = "text")


regdata%>%
  mutate(month = paste0(year,"-",month))%>%
  select(month,GS10,y_hat,y_hat2)%>%
  gather("key","val",-month)%>%
  ggplot(aes(month,val,color=key,group=key))+
  geom_line()+
  theme_minimal()
```


### Effects on Macroeconomy: FAVAR Analysis
To look at the effect of the LSAP index on macroeconomic and financial variables, I use a factor-augmented vector autoregression analysis. The macro and financial variables used for the FAVAR analysis is the same as Bernanke et al (2005). First scrape the data from FRED and Quandl to create the favar dataset. 

#### Data from FRED
```{r, eval=F}
# Get fred codes from csv file
fredmetrics <- read.csv("data/fredmetrics.csv")
# Exter API here
apikey <- "enter api here"
fredr_set_key(apikey)

# catch the fredcodes with changed names
error <- c()
for (i in 1:length(fredmetrics$fred_code)){
  try <- try(fredr(series_id=fredmetrics$fred_code[i],
                   observation_start=as.Date("1992-01-01"),
                   observation_end=as.Date("2019-12-31")))
  if("try-error" %in% class(try)){
    error <- union(error,fredmetrics$fred_code[i])
  } 
}

# Error shows ISM codes abd S&P500-PE could not be found. Get these from Quandl
error <- data.frame(error)%>%
  left_join(select(fredmetrics,fred_code,description),by=c("error"="fred_code"))

# Remove codes that could not be found before extracting data
fredmetrics <- filter(fredmetrics,!(fred_code %in% error$error))

# Extract data
freddata <- lapply(fredmetrics$fred_code, function(x)
  fredr(series_id=x,
        observation_start=as.Date("1992-01-01"),
        observation_end=as.Date("2019-12-31"),
        frequency = "m"))

freddata <- bind_rows(freddata) %>% 
  left_join(select(fredmetrics, transform_code, slow_move_code, fred_code), by = c("series_id"= "fred_code"))

write.csv(freddata, file = "data/freddata.csv")
```

```{r}
# Since data already extracted from FRED and saved, read csv file
freddata <- read.csv("data/freddata.csv")

head(freddata)

```

Some variables have missing data for few months, must check to see how to fill the gap.

#### Data from Quandl
```{r, eval=F}
quandlmetrics <- read.csv("data/quandlmetrics.csv")
Quandl.api_key("enter Quandl api here")

# Extract data, to make sure there is parity with names the series_id is the fred code
quandldata <- lapply(1:length(quandlmetrics$quandl_code), function(x)
  Quandl(quandlmetrics$quandl_code[x], 
         type = "raw", 
         start_date="1992-01-01", 
         end_date="2019-12-31",
         order = "asc")%>%
    rename_all(tolower)%>%
    select(contains("date"),contains("index"),contains("value"))%>%
    rename_at(vars(-"date"),~"value")%>%
    mutate(series_id=quandlmetrics$fred_code[x])) 

quandldata <- bind_rows(quandldata) %>% 
  left_join(select(quandlmetrics, transform_code, slow_move_code, fred_code), by = c("series_id"= "fred_code"))

write.csv(quandldata, file = "data/quandldata.csv")
```

```{r,eval=F}
# Since data already saved, read from csv
quandldata <- read.csv("data/quandldata.csv")

head(quandldata)
```

Some variables have missing data for few months, check to see how to fill gap. 

#### Combine data
```{r,eval=F}
# Combine Fred and Quandl data and transform variables using transform_code
# 1: no transformation, 2: first difference (no such variables)
# 4: log, 5: first difference of log
favardata <- bind_rows(freddata, quandldata) %>% 
  group_by(series_id) %>% 
  mutate(trans_value = case_when(transform_code == 1 ~ value, 
                                 transform_code == 4 ~ log(value),
                                 transform_code == 5 ~ log(value)-lag(log(value))),
         ym = format(as.Date(date),"%Y-%m"),
         year = year(date),
         month = month(date)) %>% 
  ungroup() %>% 
  select(ym, year, month, series_id, trans_value) 

favardata <- favardata[-c(24998,24990,25333),] %>%
  spread(series_id, trans_value) %>% 
  left_join(index_lsap, by = c("year", "month")) %>% 
  mutate(lag = na.locf(indexlsap, na.rm = F),
         index_lsap = if_else(is.na(indexlsap), round(lag, 4), round(indexlsap, 4)),
         index_lsap = if_else(is.na(index_lsap), 0, index_lsap)) %>% 
  select(-document, -doc, -indexlsap, -lag, -SP500) %>% 
  filter(year %in% c(1993:2019))

write.csv(favardata, "data/favardata.csv", row.names = F)

head(favardata)
```

#### Impute missing values
Use `r, eval=F na.interp()` function from forecast package to impute missing values in variables. This is a linear interpolation process for non-seasonal time series and periodic stl decomposition for seasonal time series. 
```{r,eval=F}
impute <- c("EXMXUS", "MHHNGSP", "NMFBAI", "NMFBI", "NMFEI", "NMFNOI", "NMFPI",
            "PPIFCG", "PPIFGS", "PPILFE", "AMBSL")
tsfavardata <- ts(favardata, start = 1993, frequency = 12)

for (i in impute) {
  tsfavardata[,i] <- na.interp(tsfavardata[,i])
}

# Check if any more NAs exist
sapply(tsfavardata, function(x) sum(is.na(x)))
```


#### FAVAR BBE (2005) Preferred Method
For the preferred specification the authors consider the policy variable, in this case the Federal funds rate, to be the only observable variable. In my model, the LSAP index is also considered a policy variable and observed by the central bank. 
```{r,eval=F}
# Scale variables: BBE pg 405, irfs reported in SD units
fitfavardata <- data.frame(tsfavardata) %>% 
  select(-ym, -year, -month) %>% 
  mutate_all(.funs = scale)

# Only FFR and LSAP index observed, remove those variables before estimating factors
preferred <- fitfavardata %>% 
  select(-FEDFUNDS, -index_lsap) 

# tspreferred <- ts(preferred, start = 1990, frequency = 12)

# Step 1: estimate factors by finding the first 3 principal components
factors <- prcomp(preferred, rank = 3)$x
eigenvec <- prcomp(preferred, rank = 3)$rotation
favar_est <- data.frame(cbind(factors, fitfavardata$FEDFUNDS, fitfavardata$index_lsap)) %>%
  rename("FEDFUNDS"="V4", "index_lsap"="V5")
favar <- irf(VAR(ts(favar_est, start = 1993, frequency = 12), p = 13), 
             impulse = "index_lsap", n.ahead = 48)
impulse <- favar$irf$index_lsap[,-c(4:5)] %*% t(eigenvec) %>% 
  as.data.frame() %>% 
  mutate(tt=1:49, 
         model = "preferred")

factors1 <- prcomp(preferred, rank = 1)$x
eigenvec1 <- prcomp(preferred, rank = 1)$rotation
favar_var1 <- data.frame(cbind(factors1, fitfavardata[,colnames(fitfavardata) %in% 
                                                        c("INDPRO", "PPIACO", "UnRate", "FEDFUNDS", "index_lsap")]))
favar_var1 <- favar_var1[,c(1,3,4,5,2,6)]
favar_1 <- irf(VAR(ts(favar_var1, start = 1993, frequency = 12), p = 13), 
             impulse = "index_lsap", n.ahead = 48)
impulse_1 <- favar_1$irf$index_lsap %>% 
  as.data.frame() %>% 
  mutate(tt=1:49,
         model = "VAR 1 factor")

impulse.plot <- bind_rows(impulse, impulse_1)

impulse.plot %>% 
  select(tt,model,INDPRO,UnRate,PPIACO,FEDFUNDS) %>% 
  gather(irf, value, -tt, -model) %>% 
  ggplot(aes(tt, value, color = model, group = model)) +
  geom_line() + 
  theme_minimal() +
  facet_wrap(~irf, scales = "free_y", ncol = 2)
```

#### FRED-MD Data
This is a large macroeconomic database consisting of ~128 monthly macroeconomic variables, prepared by McCracken and Ng (2015). This dataset contains information similar to the Stock and Watson dataset.  The authors have shown the comparibility between both datasets. More details can be obtained at [the FRED website](https://research.stlouisfed.org/econ/mccracken/fred-databases/).

```{r}
fredmd <- read.csv("data/fred-md.csv")
# Save transform codes separately
macroind <- fredmd[1,2:ncol(fredmd)] %>% 
  gather("var","transform_code")

# Some indicators only have 1 missing value. For those impute by locf
impute_locf <- c("CMRMTSPLx", "HWI", "HWIURATIO", "BUSINVx", "ISRATIOx", "NONREVSL",
                 "CONSPI", "S.P.div.yield", "CP3Mx", "COMPAPFFx", "DTCOLNVHFNM",
                 "DTCTHFNM")
# Some indicators have many missing values, for those impute by interp 
impute_interp <- c("PERMIT", "PERMITNE", "PERMITMW", "PERMITS", "PERMITW", "ACOGNO",
                   "ANDENOx", "S.P.PE.ratio", "TWEXAFEGSMTHx", "UMCSENTx", "VXOCLSx")

# Save all but row 1 of fredmd
# Impute missing values
# gather data by variable name and join transform code
fredmonthly <- fredmd[-1,] %>% 
  mutate(date = mdy(sasdate)) %>% 
  mutate_at(impute_locf, ~na.locf(.)) %>% 
  mutate_at(impute_interp, ~na.interp(.)) %>% 
  select(-sasdate) %>% 
  gather("var", "value",-date) %>% 
  left_join(macroind, by = c("var"="var"))

# Transform variables by transform code
# 1: no transformation, 2: First difference, 3: Second difference
# 4: log, 5: First difference of log, 6: second difference of log
# 7: xt/xt-1 - 1

fredmonthly <- fredmonthly %>% 
  group_by(var) %>% 
  mutate(transvalue = case_when(transform_code == 2 ~ c(NA, diff(value)),
                                transform_code == 3 ~ c(rep(NA,2), diff(value, differences = 2)),
                                transform_code == 4 ~ log(value),
                                transform_code == 5 ~ c(NA, diff(log(value))),
                                transform_code == 6 ~ c(rep(NA,2), diff(log(value), differences = 2)),
                                transform_code == 7 ~ value/lag(value)-1,
                                TRUE ~ value)) %>% 
  ungroup()

# FAVAR data
favardata <- fredmonthly %>% 
  select(date,var,transvalue) %>% 
  spread(var,transvalue) %>% 
  mutate(year = year(date),
         month = month(date)) %>% 
  left_join(index_lsap, by = c("year", "month")) %>% 
  mutate(lag = na.locf(indexlsap, na.rm = F),
         index_lsap = if_else(is.na(indexlsap), round(lag, 4), round(indexlsap, 4)),
         index_lsap = if_else(is.na(index_lsap), 0, index_lsap)) %>% 
  select(-document, -doc, -indexlsap, -lag) %>% 
  filter(year %in% c(1993:2019))
```

#### Re-run Simple Regression
```{r}
test.results <- list()
testvars <- c("INDPRO", "CPIAUCSL", "UNRATE", "index_lsap")
test.results <- lapply(c("FEDFUNDS", "GS1", "TB3MS"), function(x) run_reg("GS10", c(testvars, x), favardata))
stargazer(test.results, type = "text")
```



#### Simple VAR
$$Y_t=\Phi(L)Y_{t-1}+\varepsilon_t $$
where $Y_t={IP_t, CPI_t, u_t, FFR_t, LSAP_t}$ and $\Phi(L)$ is a lag operator $L$ being the number of lags considered.

```{r}
# Function to run VAR and compute impulse response function
# input: data and shock variable, output: impulse response
run_var <- function(data, shock){
  vardata <- data %>% 
    mutate_all(.funs = scale)
  varest <- VAR(ts(vardata, start = 1993, frequency = 12), p = 8, type = "const")
  impulse <- irf(varest, impulse = shock, 
                 n.ahead = 48, ortho = T, cumulative = F, boot = F)
  
  return(impulse)
}

var_cols <- c("INDPRO", "CPIAUCSL", "UNRATE", "FEDFUNDS", "index_lsap")
var_impulse <- run_var(favardata[,var_cols], "index_lsap")
var_impulse.plot <- data.frame(var_impulse$irf$index_lsap) %>% 
  mutate(tt = row_number()) %>% 
  gather(var, value, -tt) %>% 
  left_join(macroind, by = c("var")) %>% 
  group_by(var) %>% 
  mutate(trans_value = case_when(transform_code == 2 ~ cumsum(value),
                                 transform_code == 3 ~ cumsum(value)-lag(value),
                                 transform_code == 4 ~ exp(value)-1,
                                 transform_code == 5 ~ exp(cumsum(value))-1,
                                 transform_code == 6 ~ exp(cumsum(value)-lag(value))-1,
                                 transform_code == 7 ~ (1+value)*lag(cumsum(value)),
                                TRUE ~ value)) %>% 
  ungroup()

var_impulse.plot %>% 
  ggplot(aes(tt, trans_value, color = var, group = var)) + 
  geom_line() + 
  theme_minimal() + 
  facet_wrap(~var, scales = "free_y")

```

#### BBE(2005) Preferred Method
```{r}
# Function to run FAVAR
# Input: data, shock, #factors. Output: impulse response
run_favar <- function(data, shock, factors){
  fvardata <- data %>% 
    select(-date, -year, -month) %>%
    mutate_all(.funs = scale)
  # FEDFUNDS and index_lsap observed, hence remove before estimating factors
  unobserved <- fvardata %>% select(-FEDFUNDS, -index_lsap)
  estfactor <- prcomp(unobserved, rank = factors)$x
  esteigenvec <- prcomp(unobserved, rank = factors)$rotation
  fvardat <- data.frame(cbind(estfactor, fvardata$FEDFUNDS, fvardata$index_lsap)) %>% rename("FEDFUNDS"="V4", "index_lsap"="V5")
  
  fvarest <- VAR(ts(fvardat, start = 1993, frequency = 12), p = 8, type = "const")
  fvarimpulse <- irf(fvarest, impulse = shock, 
                 n.ahead = 48, ortho = T, cumulative = F, boot = F)
  fvarimpulse.plot <- fvarimpulse$irf$index_lsap[,-c(4:5)] %*% t(esteigenvec) %>% 
    as.data.frame() %>% 
    mutate(tt = row_number())
  
  return(fvarimpulse.plot)
}

favar_impulse.plot <- run_favar(favardata, "index_lsap", 3) %>% 
  gather(var, value, -tt) %>% 
  left_join(macroind, by = c("var")) %>% 
  group_by(var) %>% 
  mutate(trans_value = case_when(transform_code == 2 ~ cumsum(value),
                                 transform_code == 3 ~ cumsum(value)-lag(value),
                                 transform_code == 4 ~ exp(value)-1,
                                 transform_code == 5 ~ exp(cumsum(value))-1,
                                 transform_code == 6 ~ exp(cumsum(value)-lag(value))-1,
                                 transform_code == 7 ~ (1+value)*lag(cumsum(value)),
                                TRUE ~ value)) %>% 
  ungroup()

favar_impulse.plot %>% 
  filter(var %in% c("INDPRO", "CPIAUCSL", "UNRATE", "AAA", "BAA", "GS10", "TB3MS")) %>% 
  ggplot(aes(tt, trans_value, color = var, group = var)) + 
  geom_line() + 
  theme_minimal() + 
  facet_wrap(~var, scales = "free_y")
  
```

